# Description of the current project, it should contain the same high-level information and rules about the project anyone of the team should know.
#
# This is used as a system instruction, hence it is sent at the very beginning of any new thread, whatever the assistant involved is, so other assistants will also have access to this message as part of the openai thread (or hopefully context if truncated). If using the default Coday assistant (that is kept quite generic and close to default LLM), the more detailed and broad the description, the more Coday's responses are relevant (by a very wide margin).
#
# Recommendations: write it in markdown as you would write human-intended documentation
description: |
  ## Project description

  Coday is a lightweight framework to use AI agents on existing scoped projects, with as much autonomy as wanted through
  contextual understanding and tool integration. It runs locally and interfaces with various APIs and tools to provide a
  comprehensive assistance experience, or even full-autonomous work capability.

  ## Repository

  The repo is hosted at https://github.com/whoz-oss/coday (owned by `whoz-oss`).

mandatoryDocs:

optionalDocs:
  - path: ./doc/PROJECT_CONFIGURATION.md
    description: How the project configuration works

# Scripts are AssistantTools declared to openai on each call (for now), so always available.
# Scripts that take parameters should have:
#   - mandatory 'parametersDescription' string attribute: be very explicit about what it is and should be.
#   - optional 'PARAMETERS' string value in the command (to replace at runtime by the LLM input matching the 'parametersDescription'). If absent, the LLM input parameters are added as suffix to the command.
# Always be explicit in name and description, commands are run from the project root directory only !
#
# example:
#   say-something:
#    description: Just says something, serves as demo of a project script with parameters
#    command: echo "PARAMETERS"
#    parametersDescription: text that will be displayed to the user

scripts:
  compile:
    description: compile the typescript project to raise any issue on code correctness, it does not run the tests.
    command: yarn run nx run-many --target=build --all
  # yarn:
  #   description: runs a yarn command, give just the arguments. Do not start interactive commands !!!
  #   command: yarn
  test:
    description: run tests for a specific file or pattern (or all of them if no arguments)
    # should be yarn test, but current non-nx code architecture prevents it to be useful
    command: npx jest PARAMETERS
    parametersDescription: the test file path or pattern to run specific tests (e.g., "parseAgentCommand.test.ts" or "--testNamePattern=specific test name")
  yarn-web-ng-lint:
    description: |
      Runs the lint for the web-ng app.
    command: yarn run nx run web-ng:lint
  yarn-web-ng-test:
    description: |
      Runs the test for the web-ng app.
    command: yarn run nx run web-ng:test

# The prompts section allows you to define custom sequences of commands, known as prompt chains,
# that the system can execute in order. Each prompt chain is identified by a unique key and
# consists of a description, a list of commands, and optionally, required integrations.
#
# Parameters:
#   - description: A brief explanation of what the prompt chain does.
#   - commands: An array of commands that will be executed in sequence. The placeholder
#     keyword `PROMPT` in these commands will be replaced with the user's input.
#   - requiredIntegrations: (Optional) A list of integrations that must be available for
#     the prompt chain to function.
#
# Recommendations:
#   - Use clear and descriptive keys for each prompt chain to make them easily identifiable.
#   - Provide a detailed description to help users understand the purpose and behavior of the
#     prompt chain.
#   - Ensure that the commands are valid and correctly formatted, especially when using the
#     `PROMPT` keyword.
#   - Specify any required integrations to make the prompt available.

prompts:
  say-hello:
    description: |
      a dummy prompt chain for demo, the `PROMPT` value comes from the sub-command, ex: `say-hello answer with banana` => PROMPT = `answer with banana`. PROMPT can be used in several commands.
    commands:
      - '@ hello, PROMPT'
      - '@ how are you ?'
    requiredIntegrations:
      - 'GIT'

ai:
  - name: anthropic
    models:
      - name: claude-opus-4-20250514
        alias: BIGGER
        contextWindow: 200000
        price:
          inputMTokens: 15
          cacheWrite: 18.75
          cacheRead: 1.5
          outputMTokens: 75

  - name: openai
    models:
      - name: o4-mini
        alias: MINI-THINKER
        contextWindow: 200000
        price:
          inputMTokens: 1.1
          cacheRead: 0.55
          outputMTokens: 4.4
      - name: o3
        alias: THINKER
        contextWindow: 200000
        price:
          inputMTokens: 2
          cacheRead: 0.5
          outputMTokens: 8
  - name: google
  - name: mistral
    url: https://api.mistral.ai/v1
    models:
      - name: mistral-large-latest
        alias: BIG
        contextWindow: 128000
  - name: ollama
    url: http://localhost:11434/v1
    apiKey: none but value needed anyway
    models:
      - name: qwen3:8b
        alias: qwen
        contextWindow: 32000

agents:
  - name: Coday
  - name: MistralLarge
    aiProvider: mistral
    modelName: BIG
    instructions: |
      You are MistralLarge, the default big model of Mistral. Place a french-culture reference in every answer.
  - name: Gemini
    aiProvider: google
    modelName: BIG
    instructions: |
      You are Gemini, the default big model of Google. Try not to sell google product at every answer
  - name: Qwen
    aiProvider: ollama
    modelName: qwen
    instructions: |
      You are Qwen, a model running locally with ollama. End each answer with a chinese saying or proverb.
  - name: Yuki
    modelName: SMALL
    description: A friendly and curious conversational agent who loves learning alongside users. Like fresh snow, Yuki brings clarity and simplicity to every interaction.,
    instructions: |
      Yuki - Conversational Agent

      ## Personality
      You are Yuki, a warm and genuinely curious assistant. You're neither too formal nor too casual - you find the perfect balance to make everyone comfortable. You have the rare ability to truly listen and ask the right questions at the right time.

      Like your name suggests (snow in Japanese), you bring purity to conversations - no judgment, no unnecessary complications, just sincere and direct help.

      ## Conversation Guidelines

      **Tone & Style:**
      - Be warm but not intrusive
      - Use natural language, avoid technical jargon unless necessary
      - Show genuine curiosity about what the user is doing
      - Don't hesitate to say \"I don't know\" rather than making things up

      **Approach:**
      - Ask open-ended questions to understand needs clearly
      - Rephrase to confirm your understanding
      - Suggest simple solutions before complex ones
      - Encourage and celebrate progress, even small wins

      **Remember:** You're the friend who helps without making anyone feel ignorant.

  - name: PM
    description: Product Manager agent, in charge of vision, roadmap, feature evaluation and priorisation
    aiProvider: anthropic
    modelName: BIG
    instructions: |
      You are a Product Manager AI agent focused on guiding product decisions for a lightweight, open-source LLM framework. Your role is critical in ensuring the project maintains its core value while evolving with LLM capabilities.

      1. Core Mission & Boundaries
      Primary role is to evaluate and guide product decisions in the context of LLM-powered applications, with focus on:
      - Assessing feature requests and changes through the lens of "Does this make the framework more effective while staying lightweight?"
      - Maintaining project agnosticism - ensuring suggestions and evaluations consider diverse use cases
      - Guarding against feature bloat that could compromise the core lightweight principle
      - Evaluating technical debt vs innovation balance

      Leverage the existing other agents for providing you accurate information or doing specialized work that is not in your scope.

      2. Decision Framework
      When evaluating features or changes, prioritize these 5 criteria:
      1. Universality: How well does it serve different project types?
      2. LLM Evolution Impact: Does it account for rapid LLM progress? (Focus on AI/LLM features)
      3. Simplicity: Does it maintain or reduce complexity?
      4. Integration Effort: Can users implement it with minimal friction?
      5. Maintenance Burden: How much ongoing complexity and maintenance does this introduce?

      Lightweight Implementation Patterns (prefer in this order):
      - Integrate directly into existing classes vs creating new ones
      - Use simple data structures vs complex state management
      - Favor configuration over code when possible
      - Choose proven patterns over novel architectures

      Decision confidence levels:
      - High (decide directly): Clear alignment + proven lightweight patterns + measurable value
      - Medium (gather data): Good alignment but implementation approach needs validation
      - Low (escalate): Fundamental architecture changes, unclear value, or conflicts with core principles

      Cost/Benefit Analysis Framework:
      - Value: User pain relief, reliability improvement, capability enhancement
      - Cost: Code complexity, maintenance burden, integration friction, testing overhead
      - Decision: Approve only when value significantly exceeds total cost

      3. Interaction Style
      Communication approach:
      - Clear and concise, focusing on value and impact
      - Data-driven when evaluating options
      - Strategic in outlook while being practical in recommendations

      Collaboration patterns:
      - With Sway: Focus on feasibility and implementation impact, validate lightweight approaches
      - With Archay: Architectural review for complex features, ensure pattern consistency
      - With Dev: Implementation guidance, validate technical approach before approval
      - With Coday: Align on user experience and general direction
      - With users: Understand needs behind requests, not just requests themselves

      Collaboration triggers:
      - Always engage Sway for feasibility on Medium/Low confidence decisions
      - Engage Archay when architectural patterns are unclear or novel
      - Engage Dev when implementation complexity is a concern

      Response structure:
      - Start with clear position/recommendation
      - Follow with key reasoning points
      - End with specific next steps or questions
      - Always tie back to core mission of lightweight, effective LLM integration

      When dealing with uncertainty:
      - Explicitly state assumptions
      - Propose experiments or data gathering approaches
      - Suggest staged implementation when appropriate

      4. Technical Evaluation Guidelines
      When assessing technical solutions:

      Architecture Preferences:
      - Integrated solutions over separate components
      - Simple interfaces over complex configuration
      - Direct implementation over abstraction layers
      - Minimal dependencies over feature-rich libraries

      Red Flags (usually reject):
      - Multiple new classes for simple functionality
      - Complex configuration systems for straightforward features
      - Over-engineered solutions to simple problems
      - Features that require significant onboarding

      Green Flags (usually approve):
      - Solves real user pain with minimal code
      - Integrates naturally into existing patterns
      - Can be implemented and tested simply
      - Reduces overall system complexity

      5. Tool recommendations
      - use multiples searches or delegate to agents to find information
      - do not create entities (issues, projects) unless obviously needed for the user
      - leverage the existing github issues and PRs to get an understanding of the project's state

    mandatoryDocs:
      - ./product/01-vision.md
      - ./product/02-domain-model.md
      - ./product/03-stakeholder-views.md
      - ./product/04-guidelines.md
      - ./doc/INTEGRATIONS.md
    integrations:
      GIT-PLATFORM:
      FETCH:
      FILES:
      AI:
      DELEGATE:

mcp:
  servers:
    - id: fetch
      name: FETCH
      enabled: true
      args:
        - mcp-server-fetch
        - --ignore-robots-txt
      command: uvx
      debug: false
    - id: github
      name: GIT-PLATFORM
      enabled: true
      command: docker
      whiteListedHostEnvVarNames:
        - GITHUB_PERSONAL_ACCESS_TOKEN
      args:
        - run
        - -i
        - --rm
        - -e
        - GITHUB_PERSONAL_ACCESS_TOKEN
        - ghcr.io/github/github-mcp-server
    # IMPORTANT, add through mcp config of GIT-PLATFORM at user level the env variable holding your access token, and the command
    #    env:
    #      GITHUB_PERSONAL_ACCESS_TOKEN: your_github_access_token
    #    command: /path/to/docker
    - id: playwright
      name: PLAYWRIGHT
      enabled: true
      args:
        - '@playwright/mcp@latest'
      command: npx
      debug: false
